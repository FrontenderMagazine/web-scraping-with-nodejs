# Грабим (корованы) с помощью Node.js

Сграбить — програмно получить данные из интернета. С ростом объемов данных в нём
грабеж становится всё более распространённым, и для его упорщения возникло множество
мощных сервисов. К сожалению, большинство из них дороги, на них наложены некие
ограничения или обладают другими недостатками. Вместо того, что бы обращаться к
одному из сторонних сервисов **можно с помощью Node.js самостоятельно написать
мощный граббер**, который будет одновременно и очень гибким и абсолютно бесплатным.

В этой статья мы рассмотрим:

* два модуля Node.js, Request и Cheerio, которые упростят нам процес грабежа;
* тривиальное приложение, которое получает и отображает некие данные;
* более сложное приложение, которое находит ключевые слова, связанные с поисковыми
запросами в Google.

Ещё кое что, что стоит иметь в виду: **Для понимания этой статьи рекомендуется
иметь базовое представление о Node.js**; так что если вы вообще не в курсе что
это, [прочтите-ка это][1][1][2] прежде, чем продолжить. Кроме того,
пользовательское соглашение некоторых сайтов запрещает их грабить, так что стоит
прояснить этот момент, прежде чем что то делать.


## Модули

Что бы установить вышеупомянутые модули в Node.js используется [NPM][3][2][4],
Node Package Manager (если вы слышали о Bower, то это то же самое, за тем
исключением, что NPM используется для установки Bower). NPM это менеджер пакетов
который автоматически устанавливается вместе с Node.js, что бы сделать
использование пакетов максимально безболезненным. По умолчанию NPM устанавливает
модули в директорий `node_modules` в том же директорие, в котором была выполнена
команда, так что проследите за тем, что бы выполнить её в папке проекта.

И без дальнейших проволочек, вот модули, которые мы будем использовать.


### Request

В то время, как Node.js реализует простые методы скачивание данных из интернета
через HTTP и HTTPS, их нужно обрабатывать отдельно, неговоря уже о редиректах и
других вопросах, которые возникают в процесе грабежа. [Модуль Request][5][3][6]
объеденяет эти методы, позволяя абстрагироваться от сложностей, и предоставляет
интерфейс для создания запросов. Мы будем использоват этот модуль для того, что
бы скачивать страницы непосредственно в память. Что бы установить его выполните
в терминале команду `npm install request` там, где будет находится основной
Node.js скрипт.


### Cheerio

[Cheerio][7][4][8] позволяет работать со скачанными из сети данными используя
синтаксис аналогичный тому, который используется в jQuery. Цитируя текст на
главной страничке: «Cheerio это быстрый, гибкий и надежный порт jQuery,
разработанный специально для сервера». Использование Cheerio позволяет
сконцентрироваться непосредственно на работе с полученными данными, а не на их
парсинге. Для установки выполните в терминале команду `npm install cheerio`
в директорие, в котором будет находится основной Node.js скрипт.


## Имплементация

Код, приведенный ниже, это простое приложение, которое получает температуру
с сайта погоды. Я добавил в конце URL код моей страны, но, если хотите, можете
добавить туда код своей (главное убедитесь, что установили модули, которые мы
пытаемся подключить; вы можете узнать как это сделать по ссылкам приведенным выше)

    var request = require("request"),
    	cheerio = require("cheerio"),
    	url = "http://www.wunderground.com/cgi-bin/findweather/getForecast?&query=" + 02888;

    request(url, function (error, response, body) {
    	if (!error) {
    		var $ = cheerio.load(body),
    			temperature = $("[data-variable='temperature'] .wx-value").html();

    		console.log("Температура " + temperature + " градусов по Фаренгейту.");
    	} else {
    		console.log("Произошла ошибка: " + error);
    	}
    });

Так, и что мы тут делаем? Сначала подключаем модули, что бы можно было их
использовать в скрипте. Потом сохраняем в переменной URL, который грабим.

Затем используем модуль Request, что бы скачать страничку, находящуюся по
переданному функции `request` URL. Мы передаем в качестве аргументов URL,
который хотим скачать и коллбек, в котором обработаем результаты запроса.
Когда мы получим данные, будет вызван колбек в который будут переданы в качестве
аргументов три переменных: `error`, `responce` и `body`. Если Request не может
скачать страничку и получить данные, он передаст в функцию объект в переменной
`error` и `null` в качестве аргумента `body`. Прежде, чем начать работать
с данными проверяем не произошла ли ошибка; если произошла — выводим сообщение
в консоль, что бы увидеть, что пошло не так.

Если все хорошо, то передаем данные Cheerio. В результате мы можем работать с
данными как на любой веб-страничке, используя стандартный синтаксис jQuery.
Что бы найти нужные нам данные, надо написать селектор, который получит нужный
элемент или элементы на странице. Если вы откроете в браузере URL, использованный
в примере и исследуете страничку с помощью инструментов разработчика, то заметите,
что большой, зеленый элемент с темературой на нем — тот самый элемент, для
которого написан селектор. Теперь, когда мы нашли нужный элемент осталось просто
получить данные и вывести в консоль.

Отсюда можно двигаться во множестве направлений. Поиграйте с кодом,
я привел основные шаги ниже. А именно:


### В браузере

1. Откройте страничку, которую хотите сграбить, сохраните URL.
2. Определите элемент или элементы, данные из которых вы хотите получить и
   jQuery-селектор, который позволит получить эти элементы.


### В коде

1. Используйте Request что бы скачать страницу находящуюся по выбранному вами URL.
2. Передайте полученные данные Cheerio, что бы использовать jQuery-подобный
   интерфейс.
3. Используйте селектор, написанный ранее, что бы сграбить данные со страницы.


## Идем дальше: дата-майнинг

Более продвинутое использование грабберов можно отнести к [дата-майнингу][9][5][10],
процессу скачивания страниц и генерации отчетов на основе полученных данных.
Node.js отлично подходит для создания подобных приложений.

Я написал на Node.js небольшое, меньше сотни строк кода, приложение для
дата-майнинга, что бы показать как использовать две упомянутые ранее библиотеки
менее тривиальным образом. Приложение ищет наиболее популярные ключевые слова,
ассоциированные с определенным поисковым запросом Google, анализируя текст
каждой страницы, на которую ссылается первая страница результатов Google.

В приложении три главных фазы:

1. Проанализировать поисковую выдачу Google.
2. Скачать все страницы и распарсить текст на каждой из них.
3. Проанализировать текст и представить наиболее популярные слова.

Давайте взглянем на код, который позволит реализовать каждый из этих пунктов —
как вы можете догадаться, его совсем немного.


### Скачиваем поисковую выдачу Google

Первое, что нужно сделать — определить какую страничку мы собираемся
анализировать. Так как мы собираемся анализировать странички из поисковой выборки
Google, просто используем URL с нужным поисковым запросом, скачиваем и парсим
результаты, что бы получить нужные URL.

Что бы скачать странички — используем Request, как и в примере выше и парсим его
используя Cheerio. Вот как выглядит код:

    request(url, function (error, response, body) {
    	if (error) {
    		console.log(“Не удалось получить страницу из за следующей ошибки: “ + error);
    		return;
    	}

      // загружаем тело страницы в Cheerio что бы можно было работать с DOM
    	var $ = cheerio.load(body),
    		links = $(".r a");

    	links.each(function (i, link) {
        // получаем атрибуты href для каждой ссылки
    		var url = $(link).attr("href");

        // обрезаем ненужный мусор
    		url = url.replace("/url?q=", "").split("&")[0];

    		if (url.charAt(0) === "/") {
    			return;
    		}

        // ссылка считается результатом, так что увеличиваем их количество
    		totalResults++;

В этом случае, переменная с URL, который мы передаем, поисковый запрос для
термина «data mining».

Как видите, мы сначала делаем запрос, что бы получить содержимое страницы. Затем
мы загружаем содержимое страницы в Cheerio что бы иметь возможность получать
элементы, содержащие релевантные запросу ссылки, из DOМ. Затем мы перебираем
ссылки и обрезаем ненужные нам параметры URL, которые добавляет Google для
своих целей — когда мы скачиваем страницы с помощью модуля Request, они нам
не нужны.

Наконец, когда мы все это сделаем, нужно убедится что URL не начинается с `/` — 
если это так, это внутренняя ссылка на ресурс Google и нам её скачивать не нужно,
так как либо ссылка нам не подходит.


### Получение слов с каждой страницы

Теперь, когда у нас есть URL страниц, надо получить слова с каждой из них. Этот
шаг представляе собой ровно то же, что мы уже делали ранее — только теперь, URL
относится к URL страницы, которую мы нашли и обработали в цикле ранее.

    request(url, function (error, response, body) {
      // Загружаем страницу в Cheerio
    	var $page = cheerio.load(body),
    		text = $page("body").text();

Снова, используем Request и Cheerio, что бы скачать страницу и получить доступ к
её DOM. В данном случаем мы получем только текст со страницы.

Далее, нам нужно очистить текст со страницы — там будет всяческий мусор, который
нам не нужен, например масса лишних пробелов, стили, иногда даже немного данных
JSON. Вот что нам надо сделать:

1. Сжать все пробелы в один.
2. Выбрасываем все символы, которые не являются буквами или пробелами.
3. Переводим все в нижний регистр.

Когда мы это сделаем можно будет просто разбить наш текст по пробелам и у нас
окажется массив содержащий все отображенные слова на странице. Мы можем перебрать
его в цикле и добавить их к словарю.

Код, который всё это делает выглядит приблизительно так:

    // Избавляемся от лишних пробелов и нечисловые символы.
    text = text.replace(/\s+/g, " ")
    	     .replace(/[^a-zA-Z ]/g, "")
    	     .toLowerCase();

    // Разбиваем по пробелу, что бы получить список слов на странице и
    // перебираем их в цикле.
    text.split(" ").forEach(function (word) {
      // Скорее всего нам не нужно включать слишком короткие или длинные слова,
      // так как они, скорее всего, содержат бесполезные для нас данные.
    	if (word.length  20) {
    		return;
    	}

    	if (corpus[word]) {
        // Если слово уже находится в словаре, нашей коллекции
        // терминов, увеличиваем количество его вхождений на единицу.
    		corpus[word]++;
    	} else {
        // В противном случае, считаем, что встречаем его впервые.
    		corpus[word] = 1;
    	}
    });



### Анализируем слова

Когда все слова добавлены в словарь, мы можем перебрать их в цикле и сортировать
их по популярности. Сперва, надо внести их в массив, так как словарь — объект.

    // поместим все слова в словарь
    for (prop in corpus) {
    	words.push({
    		word: prop,
    		count: corpus[prop]
    	});
    }

    // сортируем массив основываясь на частоте вхождения слов
    words.sort(function (a, b) {
    	return b.count - a.count;
    });


The result will be a sorted array representing exactly how often each word in
it has been used on all of the websites from the first page of results of the
Google search. Below is a sample set of results for the term “data mining.” (
Coincidentally, I used this list to generate the word cloud at the top of this
article.)

Результатом будет отсортированный массив, показывающий как часто каждое слово
в нем используется на сайтах с первой страницы поисковой выдачи Google. Ниже — 
простой список результатов для термина «data mining». (По некому совпадению
я использовал этот список, что бы сгенерировать облако тегов вначале статьи).

    [ { word: 'data', count: 981 },
      { word: 'mining', count: 531 },
      { word: 'that', count: 187 },
      { word: 'analysis', count: 120 },
      { word: 'information', count: 113 },
      { word: 'from', count: 102 },
      { word: 'this', count: 97 },
      { word: 'with', count: 92 },
      { word: 'software', count: 81 },
      { word: 'knowledge', count: 79 },
      { word: 'used', count: 78 },
      { word: 'patterns', count: 72 },
      { word: 'learning', count: 70 },
      { word: 'example', count: 70 },
      { word: 'which', count: 69 },
      { word: 'more', count: 68 },
      { word: 'discovery', count: 67 },
      { word: 'such', count: 67 },
      { word: 'techniques', count: 66 },
      { word: 'process', count: 59 } ]


Если интересно взглянуть на остальной код, посмотрите
[полностью комментированный исходный код][11][6][12].

Интересным опытом было бы поднять это приложение на новый уровень. Можете
оптимизировать анализ текста, расширить поиск на множество страниц поисковой
выдачи, удалить слова, которые не являются ключевыми (такие как «that» и «from»).
Улучшить обработку ошибок, что бы сделать приложение ещё более точным — когда вы
занимаетесь дата-майнингом, то лучше иметь столько слоев проверок, сколько
возможно. Во всем том многообразии данных, которое можно получить таким образом,
неизбежно найдется блок текста, который, не будучи обработанным должным образом,
неприменно приведет к возникновению ошибки и аварийному завершению работы
приложения.


## Заключение

Как всегда, если вы нашли что то, касательно того, как грабить сайты с помощью
Node.js, которое вы считаете полезной или есть вопросы — дайте знать в
комментариях. А ещё можете читать меня в [Twitter][13][7][14] и посматривать
мой [блог][15][8][16] на предмет новых статей о Node.js, грабеже и JavaScript
в целом.

### Сноски

 [1]: http://nodejs.org/
 [2]: http://www.smashingmagazine.com/2015/04/08/web-scraping-with-nodejs/#1
 [3]: https://www.npmjs.com/
 [4]: http://www.smashingmagazine.com/2015/04/08/web-scraping-with-nodejs/#2
 [5]: https://github.com/request/request
 [6]: http://www.smashingmagazine.com/2015/04/08/web-scraping-with-nodejs/#3
 [7]: https://github.com/cheeriojs/cheerio
 [8]: http://www.smashingmagazine.com/2015/04/08/web-scraping-with-nodejs/#4
 [9]: http://en.wikipedia.org/wiki/Data_mining
 [10]: http://www.smashingmagazine.com/2015/04/08/web-scraping-with-nodejs/#5
 [11]: https://gist.github.com/elliotbonneville/1bf694b8c83f358e0404
 [12]: http://www.smashingmagazine.com/2015/04/08/web-scraping-with-nodejs/#6
 [13]: https://twitter.com/bovenille
 [14]: http://www.smashingmagazine.com/2015/04/08/web-scraping-with-nodejs/#7
 [15]: http://heyjavascript.com
 [16]: http://www.smashingmagazine.com/2015/04/08/web-scraping-with-nodejs/#8
